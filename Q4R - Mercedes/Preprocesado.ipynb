{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\BIETOSIK\\AppData\\Local\\Microsoft\\WindowsApps\\python3.11.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/BIETOSIK/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL del sitio de SharePoint\n",
    "site_url = \"https://grupoayesaa41015322-my.sharepoint.com/personal/ai_moreno_ibermatica_com/\"\n",
    "\n",
    "# Usuario y contraseña para la autenticación\n",
    "username = \"BIETOSIK\"\n",
    "password = \"Iber.1859\"\n",
    "\n",
    "# Crear el objeto de autenticación\n",
    "auth = HTTPBasicAuth(username, password)\n",
    "\n",
    "# Crear el objeto de sitio SharePoint\n",
    "site = Site(site_url, auth=auth)\n",
    "\n",
    "# Conectar al sitio\n",
    "site.auth.login()\n",
    "\n",
    "# Ruta de la carpeta en SharePoint\n",
    "folder_path = \"/Documents/Q4Real/MB/Montabilidad_Pro\"\n",
    "\n",
    "# Obtener la lista de archivos en la carpeta\n",
    "folder_files = site.get_folder(folder_path).files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> *En este cuaderno de jupiter, se expone todo el código de preprocesado necesario para la detección de exóticos con tecnología cuántica.*\n",
    "\n",
    "1. Se leen los ficheros .txt de los directorios  [test](https://grupoayesaa41015322-my.sharepoint.com/personal/ai_moreno_ibermatica_com/_layouts/15/onedrive.aspx?ct=1700555147845&or=OWA%2DNT&cid=5b628751%2D7368%2Db585%2D7884%2D814a07203a8f&fromShare=true&ga=1&id=%2Fpersonal%2Fai%5Fmoreno%5Fibermatica%5Fcom%2FDocuments%2FQ4Real%2FMB%2FMontabilidad%5FPro%2FDatos%5FTest) y [train](https://grupoayesaa41015322-my.sharepoint.com/personal/ai_moreno_ibermatica_com/_layouts/15/onedrive.aspx?ct=1700555147845&or=OWA%2DNT&cid=5b628751%2D7368%2Db585%2D7884%2D814a07203a8f&fromShare=true&ga=1&id=%2Fpersonal%2Fai%5Fmoreno%5Fibermatica%5Fcom%2FDocuments%2FQ4Real%2FMB%2FMontabilidad%5FPro%2FDatos%5FTrain) se genera un nuevo campo **\"Set\"** con el valor correspondiente; '*Test*' o '*Train*', para cada pedido del fichero. Se unen los dos ficheros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Se leen los ficheros txt\n",
    "test_csv = pd.read_csv('TV150GP_test.txt', delimiter='\\t', names = ['Pedidos'])\n",
    "train_csv = pd.read_csv('TV150GP_training.txt', delimiter='\\t',names = ['Pedidos'])\n",
    "\n",
    "#Se añade la columna 'Set'\n",
    "test_csv['Set'] = 'Test'\n",
    "train_csv['Set'] = 'Train'\n",
    "\n",
    "dataset_trabajo = pd.concat([test_csv,train_csv])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mediante la partición en subcadenas de cada linea del dataset de trabajo, se crean nuevas columnas  \n",
    "   * Código -> 0 - 40\n",
    "   * Expediente -> 44 - 58\n",
    "   * Texto -> 55 - 1055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trabajo['Codigo'] = dataset_trabajo['Pedidos'].str.slice(0, 40)  \n",
    "dataset_trabajo['Expediente'] = dataset_trabajo['Pedidos'].str.slice(44, 58)   \n",
    "dataset_trabajo['Texto'] = dataset_trabajo['Pedidos'].str.slice(55,1055)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Se vectoriza el campo texto en base al algortimo TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el vectorizador TF-IDF\n",
    "vectorizador_tfidf = TfidfVectorizer()\n",
    "\n",
    "# Aplicar el vectorizador TF-IDF a la columna 'Codigos'\n",
    "matriz_tfidf = vectorizador_tfidf.fit_transform(dataset_trabajo['Texto'])\n",
    "\n",
    "# Crear un DataFrame con los valores TF-IDF\n",
    "nombres_columnas = vectorizador_tfidf.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(matriz_tfidf.toarray(), columns=list(map(str.upper, nombres_columnas)))\n",
    "\n",
    "dataset_trabajo = dataset_trabajo.reset_index(drop=True)\n",
    "\n",
    "# Concatenar el DataFrame original con el DataFrame TF-IDF\n",
    "dataset_trabajo = pd.concat([dataset_trabajo, df_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Se parte en subcadenas la columna Codigo, se crean nuevas columnas  \n",
    "   * Tipo_vehiculo -> 18-21 + 21 -22 + 24-25\n",
    "   * N_Produccion -> 0-7\n",
    "   * N_Pedido -> 8 -18\n",
    "   * Ficha_Tecnica -> 32 - 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trabajo['Tipo_vehiculo'] = dataset_trabajo['Codigo'].str.slice(18, 21)  + '-'+ dataset_trabajo['Codigo'].str.slice(21, 22) + '-'+ dataset_trabajo['Codigo'].str.slice(24, 25)\n",
    "dataset_trabajo['N_Produccion'] = dataset_trabajo['Codigo'].str.slice(0, 7)   \n",
    "dataset_trabajo['N_Pedido'] = dataset_trabajo['Codigo'].str.slice(8,18)\n",
    "dataset_trabajo['Ficha_Tecnica'] = dataset_trabajo['Codigo'].str.slice(32,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Se genera el atributo Chasis a partir de la columna Pedidos:\n",
    "   * Chasis -> 26 - 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trabajo['Chasis'] = dataset_trabajo['Pedidos'].str.slice(26, 32)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan los codigos que no son necesarios indicados en el excel [Codigos a Excluir](https://grupoayesaa41015322-my.sharepoint.com/:x:/r/personal/ai_moreno_ibermatica_com/Documents/Q4Real/MB/Montabilidad_Pro/Datos_Train/Codigos%20a%20Excluir.xlsx?d=wf2433e0e674b4fda9944e88507bfeb63&csf=1&web=1&e=vuLREx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_codigos_excluir = pd.read_excel('Codigos a Excluir.xlsx')\n",
    "\n",
    "# Obtener la lista de columnas a eliminar\n",
    "columnas_a_eliminar = data_codigos_excluir['Codigo'].tolist()\n",
    "\n",
    "# Eliminar las columnas del primer DataFrame que coincidan con los códigos del segundo DataFrame\n",
    "dataset_trabajo = dataset_trabajo.drop(columnas_a_eliminar, axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agrupan los pedidos por categorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizar_fila(fila):\n",
    "    tipo_vehiculo = fila['Tipo_vehiculo']\n",
    "    texto = fila['Texto']\n",
    "\n",
    "    if \"447-6\" in tipo_vehiculo and \"Z2K\" in texto:\n",
    "        return \"BASE\"\n",
    "    elif \"447-6\" in tipo_vehiculo and \"Z3K\" in texto:\n",
    "        return \"PRO\"\n",
    "    elif \"447-6\" in tipo_vehiculo and \"Z4K\" in texto:\n",
    "        return \"SELECT\"\n",
    "    elif \"447-7\" in tipo_vehiculo and \"ZK4\" in texto:\n",
    "        return \"MIXTO\"\n",
    "    elif \"447-7\" in tipo_vehiculo and \"ZK5\" in texto:\n",
    "        return \"TOURIER\"\n",
    "    elif \"447-7\" in tipo_vehiculo and \"ZQ8\" in texto:\n",
    "        return \"TOURER PRO\"\n",
    "    elif \"447-7\" in tipo_vehiculo and \"ZQ9\" in texto:\n",
    "        return \"TOURER SELECT\"\n",
    "    elif \"447-8\" in tipo_vehiculo and \"VQ9\" in texto:\n",
    "        return \"CLASE V\"\n",
    "    elif \"447-8\" in tipo_vehiculo and \"VR9\" in texto:\n",
    "        return \"STYLE\"\n",
    "    elif \"447-8\" in tipo_vehiculo and \"VQ1\" in texto:\n",
    "        return \"AVANTGARDE\"\n",
    "    elif \"447-8\" in tipo_vehiculo and \"VQ8\" in texto:\n",
    "        return \"EXCLUSIVE\"\n",
    "    elif \"447-8\" in tipo_vehiculo and \"ZE9\" in texto:\n",
    "        return \"AVANTGARDE LINE ELECTRIC\"\n",
    "    elif \"447-8\" in tipo_vehiculo and \"ZK7\" in texto:\n",
    "        return \"MARCO POLO\"\n",
    "    else:\n",
    "        return \"NADA\"\n",
    "\n",
    "# Aplicar la función a cada fila del DataFrame y crear una nueva columna 'Categoria'\n",
    "dataset_trabajo['Categoria'] = dataset_trabajo.apply(categorizar_fila, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del excel [Exóticos Validados](https://grupoayesaa41015322-my.sharepoint.com/:x:/r/personal/ai_moreno_ibermatica_com/Documents/Q4Real/MB/Montabilidad_Pro/Datos_Test/Exoticos_Validados.xlsx?d=w76955712c5734d20a67dcc0432fb8df6&csf=1&web=1&e=CqmQuH) se comprueba si en el dataset de trabajo hay algun pedido exotico no validado con la etiqueta 'Train', si la hay se le pone la etiqueta 'Test' ya que no ha sido validado aun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_exoticos = pd.read_excel('Exoticos_Validados.xlsx')\n",
    "\n",
    "# Modificar directamente la columna Set en dataset_trabajo\n",
    "dataset_trabajo.loc[dataset_trabajo['N_Pedido'].isin(data_exoticos['ANR']), 'Set'] = 'Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparar con el fichero del 15 y actualizar chasis al pedido si no lo tiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_15 = pd.read_csv('TV150GP_ST15.txt', delimiter='\\t', names = ['Pedidos'])\n",
    "\n",
    "dataset_15['Chasis'] = dataset_15['Pedidos'].str.slice(26, 32)  \n",
    "dataset_15['Codigo'] = dataset_15['Pedidos'].str.slice(0, 40)  \n",
    "dataset_15['N_Pedido'] = dataset_15['Codigo'].str.slice(8,18)\n",
    "\n",
    "# Actualizar los valores de la columna Chasis en dataset_trabajo\n",
    "for index, row in dataset_trabajo.iterrows():\n",
    "    n_pedido = row['N_Pedido']\n",
    "    chasis_15 = dataset_15.loc[dataset_15['N_Pedido'] == n_pedido, 'Chasis']\n",
    "    if not chasis_15.empty:\n",
    "        # Verificar si la Serie no está vacía\n",
    "        chasis_valor = chasis_15.iloc[0]\n",
    "        dataset_trabajo.at[index, 'Chasis'] = chasis_valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportar fichero de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trabajo.to_csv('Dataset_Quantum.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se generan los .csv para Multiverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva columna 'QuantumInspire_ID' como índice\n",
    "dataset_trabajo['QuantumInspire_ID'] = range(1, len(dataset_trabajo) + 1)\n",
    "\n",
    "# Crear un nuevo DataFrame con las columnas especificadas\n",
    "df_quantum_ispire_previo = dataset_trabajo[['N_Pedido','Categoria', 'Set', 'Texto', 'Tipo_vehiculo', 'QuantumInspire_ID']].copy()\n",
    "\n",
    "# Establecer 'QuantumInspire_ID' como índice\n",
    "df_quantum_ispire_previo.set_index('QuantumInspire_ID', inplace=True)\n",
    "df_quantum_ispire_anonimizado = df_quantum_ispire_previo[['Categoria', 'Set', 'Texto', 'Tipo_vehiculo']].copy()\n",
    "\n",
    "# Exportar el DataFrame a un archivo CSV\n",
    "df_quantum_ispire_previo.to_csv('Montabilidad_to_quantum_inspire_Previo.csv')\n",
    "df_quantum_ispire_anonimizado.to_csv('Montabilidad_to_quantum_inspire_Anom.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
